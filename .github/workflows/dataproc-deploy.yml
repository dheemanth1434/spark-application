name: dataproc-deploy

on:
  workflow_dispatch:
    inputs:
      cluster:
        description: 'Cluster Name'     
        required: true
      class:
        description: 'Class Path'     
        required: true
      args:
        description: 'Arguments'     
        required: true
      
jobs:
  
  Deploy:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2
    - name: Set up JDK 1.8
      uses: actions/setup-java@v1
      with:
        java-version: 1.8
    - name: Cache Maven packages
      uses: actions/cache@v2
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2  
    - name: Build with Maven
      run: mvn -B package -Dmaven.test.skip=true --file pom.xml
    - name: copy the jar to staging folder
      run: mkdir staging && cp target/*.jar staging && mv staging/*.jar staging/artifact.jar
    - name: Setup google cloud sdk
      uses: GoogleCloudPlatform/github-actions/setup-gcloud@master
      with:
        version: 'latest'
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GCP_SA_KEY }}
        export_default_credentials: true
    - name : copy the artifact to cloud storage
      run: gsutil cp staging/*.jar gs://dp-artifacts/
    - name: submit the job to dataproc cluster
      run: gcloud dataproc jobs submit spark --cluster ${{ github.event.inputs.cluster }} --class ${{ github.event.inputs.class }} --jar staging/*.jar -- ${{ github.event.inputs.args }}
